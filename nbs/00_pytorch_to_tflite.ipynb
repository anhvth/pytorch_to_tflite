{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pytorch_to_tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PytorchToTflite\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import onnx\n",
    "import os\n",
    "import onnxruntime as rt\n",
    "\n",
    "# from converter import *\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import onnx\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from PIL import Image\n",
    "from onnx_tf.backend import prepare\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def get_example_input(size):\n",
    "    \"\"\"\n",
    "    Loads image from disk and converts to compatible shape.\n",
    "    :param image_file: Path to single image file\n",
    "    :return: Original image, numpy.ndarray instance image, torch.Tensor image\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    image = []\n",
    "    for i in range(3*size*size):\n",
    "        image.append(i%256)\n",
    "\n",
    "    image = np.array(image).astype('uint8').reshape([3, size, size])\n",
    "    image = image[None]\n",
    "    image = image.astype(np.float32)\n",
    "    return image, image, image\n",
    "\n",
    "\n",
    "# ------------------ Convert Functions ------------------ #\n",
    "def torch_to_onnx(torch_path, onnx_path, image_path):\n",
    "    \"\"\"\n",
    "    Converts PyTorch model file to ONNX with usable op-set\n",
    "    :param torch_path: Torch model path to load\n",
    "    :param onnx_path: ONNX model path to save\n",
    "    :param image_path: Path of test image to use in export progress\n",
    "    \"\"\"\n",
    "    pytorch_model = get_torch_model(torch_path)\n",
    "    image, tf_lite_image, torch_image = get_example_input(image_path)\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model=pytorch_model,\n",
    "        args=torch_image,\n",
    "        f=onnx_path,\n",
    "        verbose=False,\n",
    "        export_params=True,\n",
    "        do_constant_folding=False,  # fold constant values for optimization\n",
    "        input_names=['input'],\n",
    "        opset_version=10,\n",
    "        output_names=['output'])\n",
    "\n",
    "\n",
    "def onnx_to_tf(onnx_path, tf_path):\n",
    "    \"\"\"\n",
    "    Converts ONNX model to TF 2.X saved file\n",
    "    :param onnx_path: ONNX model path to load\n",
    "    :param tf_path: TF path to save\n",
    "    \"\"\"\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "\n",
    "    onnx.checker.check_model(onnx_model)  # Checks signature\n",
    "    tf_rep = prepare(onnx_model)  # Prepare TF representation\n",
    "    tf_rep.export_graph(tf_path)  # Export the model\n",
    "\n",
    "\n",
    "def tf_to_tf_lite(tf_path, tf_lite_path):\n",
    "    \"\"\"\n",
    "    Converts TF saved model into TFLite model\n",
    "    :param tf_path: TF saved model path to load\n",
    "    :param tf_lite_path: TFLite model path to save\n",
    "    \"\"\"\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)  # Path to the SavedModel directory\n",
    "    tflite_model = converter.convert()  # Creates converter instance\n",
    "    with open(tf_lite_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "\n",
    "\n",
    "# ------------------ Model Load Functions ------------------ #\n",
    "def get_torch_model(model_path):\n",
    "    \"\"\"\n",
    "    Loads state-dict into model and creates an instance\n",
    "    :param model_path: State-dict path to load PyTorch model with pre-trained weights\n",
    "    :return: PyTorch model instance\n",
    "    \"\"\"\n",
    "    model = torch.load(model_path, map_location='cpu')\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_tf_lite_model(model_path):\n",
    "    \"\"\"\n",
    "    Creates an instance of TFLite CPU interpreter\n",
    "    :param model_path: TFLite model path to initialize\n",
    "    :return: TFLite interpreter\n",
    "    \"\"\"\n",
    "    interpret = tf.lite.Interpreter(model_path)\n",
    "    interpret.allocate_tensors()\n",
    "    return interpret\n",
    "\n",
    "\n",
    "# ------------------ Inference Functions ------------------ #\n",
    "def predict_torch(model, image):\n",
    "    \"\"\"\n",
    "    Torch model prediction (forward propagate)\n",
    "    :param model: PyTorch model\n",
    "    :param image: Input image\n",
    "    :return: Numpy array with logits\n",
    "    \"\"\"\n",
    "    return model(image).data.cpu().numpy()\n",
    "\n",
    "\n",
    "def predict_tf_lite(model, image):\n",
    "    \"\"\"\n",
    "    TFLite model prediction (forward propagate)\n",
    "    :param model: TFLite interpreter\n",
    "    :param image: Input image\n",
    "    :return: Numpy array with logits\n",
    "    \"\"\"\n",
    "    input_details = model.get_input_details()\n",
    "    output_details = model.get_output_details()\n",
    "    model.set_tensor(input_details[0]['index'], image)\n",
    "    model.invoke()\n",
    "    tf_lite_output = model.get_tensor(output_details[0]['index'])\n",
    "    return tf_lite_output\n",
    "\n",
    "\n",
    "def calc_error(res1, res2, verbose=False):\n",
    "    \"\"\"\n",
    "    Calculates specified error between two results. In here Mean-Square-Error and Mean-Absolute-Error calculated\"\n",
    "    :param res1: First result\n",
    "    :param res2: Second result\n",
    "    :param verbose: Print loss results\n",
    "    :return: Loss metrics as a dictionary\n",
    "    \"\"\"\n",
    "    mse = ((res1 - res2) ** 2).mean(axis=None)\n",
    "    mae = np.abs(res1 - res2).mean(axis=None)\n",
    "    metrics = {'mse': mse, 'mae': mae}\n",
    "    if verbose:\n",
    "        print(f\"\\n\\nMean-Square-Error between predictions:\\t{metrics['mse']}\")\n",
    "        print(f\"Mean-Square-Error between predictions:\\t{metrics['mae']}\\n\\n\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ------------------ Main Convert Function ------------------#\n",
    "def predict_onnx(onnx_path, sample):\n",
    "    sess = rt.InferenceSession(onnx_path)\n",
    "    onnx_result = sess.run(None, {'input': sample})\n",
    "    onnx_result = sorted(onnx_result, key=lambda x:x.shape)\n",
    "    return onnx_result\n",
    "\n",
    "def onnx_to_keras(onnx_path, tf_path):\n",
    "    from pytorch2keras import pytorch_to_keras\n",
    "    from onnx2keras import onnx_to_keras as _onnx_to_keras\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "\n",
    "    onnx.checker.check_model(onnx_model)  # Checks signature\n",
    "    k_model = _onnx_to_keras(onnx_model=onnx_model, input_names=[onnx_model.graph.input[0].name],\n",
    "                                input_shapes=None, name_policy=None,\n",
    "                                verbose=True, change_ordering=None)\n",
    "    k_model.summary()\n",
    "    return k_model\n",
    "\n",
    "def predict_tf_lite(model, image):\n",
    "    \"\"\"\n",
    "    TFLite model prediction (forward propagate)\n",
    "    :param model: TFLite interpreter\n",
    "    :param image: Input image\n",
    "    :return: Numpy array with logits\n",
    "    \"\"\"\n",
    "    input_details = model.get_input_details()\n",
    "    output_details = model.get_output_details()\n",
    "    model.set_tensor(input_details[0]['index'], image)\n",
    "    model.invoke()\n",
    "    # n_outputs = len(output_details)\n",
    "    outputs = []\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    for i in range(len(output_details)):\n",
    "        tf_lite_output = model.get_tensor(output_details[i]['index'])\n",
    "        outputs += [tf_lite_output]\n",
    "    outputs = sorted(outputs, key=lambda x: x.shape)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch to Onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# NanoDet-g-416 is designed for edge NPU, GPU or TPU with high parallel computing power but low memory bandwidth\r\n",
      "# COCO mAP(0.5:0.95) = 22.9\r\n",
      "# Flops = 4.2B\r\n",
      "# Params = 3.8M\r\n",
      "# COCO pre-trained weight link: https://drive.google.com/file/d/10uW7oqZKw231l_tr4C1bJWkbCXgBf7av/view?usp=sharing\r\n",
      "save_dir: workspace/nanodet_g\r\n",
      "model:\r\n",
      "  arch:\r\n",
      "    name: OneStageDetector\r\n",
      "    backbone:\r\n",
      "      name: CustomCspNet\r\n",
      "      net_cfg: [[ 'Conv', 3, 32, 3, 2],  # 1/2\r\n",
      "                [ 'MaxPool', 3, 2 ],  # 1/4\r\n",
      "                [ 'CspBlock', 32, 1, 3, 1 ],  # 1/4\r\n",
      "                [ 'CspBlock', 64, 2, 3, 2 ],  # 1/8\r\n",
      "                [ 'CspBlock', 128, 2, 3, 2 ],  # 1/16\r\n",
      "                [ 'CspBlock', 256, 3, 3, 2 ]]  # 1/32\r\n",
      "      out_stages: [3,4,5]\r\n",
      "      activation: LeakyReLU\r\n",
      "    fpn:\r\n",
      "      name: PAN\r\n",
      "      in_channels: [128, 256, 512]\r\n",
      "      out_channels: 128\r\n",
      "      start_level: 0\r\n",
      "      num_outs: 3\r\n",
      "    head:\r\n",
      "      name: NanoDetHead\r\n",
      "      num_classes: 80\r\n",
      "      conv_type: Conv\r\n",
      "      activation: LeakyReLU\r\n",
      "      input_channel: 128\r\n",
      "      feat_channels: 128\r\n",
      "      stacked_convs: 1\r\n",
      "      share_cls_reg: True\r\n",
      "      octave_base_scale: 8\r\n",
      "      scales_per_octave: 1\r\n",
      "      strides: [8, 16, 32]\r\n",
      "      reg_max: 10\r\n",
      "      norm_cfg:\r\n",
      "        type: BN\r\n",
      "      loss:\r\n",
      "        loss_qfl:\r\n",
      "          name: QualityFocalLoss\r\n",
      "          use_sigmoid: True\r\n",
      "          beta: 2.0\r\n",
      "          loss_weight: 1.0\r\n",
      "        loss_dfl:\r\n",
      "          name: DistributionFocalLoss\r\n",
      "          loss_weight: 0.25\r\n",
      "        loss_bbox:\r\n",
      "          name: GIoULoss\r\n",
      "          loss_weight: 2.0\r\n",
      "data:\r\n",
      "  train:\r\n",
      "    name: coco\r\n",
      "    img_path: coco/train2017\r\n",
      "    ann_path: coco/annotations/instances_train2017.json\r\n",
      "    input_size: [416,416] #[w,h]\r\n",
      "    keep_ratio: True\r\n",
      "    pipeline:\r\n",
      "      perspective: 0.0\r\n",
      "      scale: [0.6, 1.4]\r\n",
      "      stretch: [[1, 1], [1, 1]]\r\n",
      "      rotation: 0\r\n",
      "      shear: 0\r\n",
      "      translate: 0.2\r\n",
      "      flip: 0.5\r\n",
      "      brightness: 0.2\r\n",
      "      contrast: [0.6, 1.4]\r\n",
      "      saturation: [0.5, 1.2]\r\n",
      "      normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\r\n",
      "  val:\r\n",
      "    name: coco\r\n",
      "    img_path: coco/val2017\r\n",
      "    ann_path: coco/annotations/instances_val2017.json\r\n",
      "    input_size: [416,416] #[w,h]\r\n",
      "    keep_ratio: True\r\n",
      "    pipeline:\r\n",
      "      normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\r\n",
      "device:\r\n",
      "  gpu_ids: [0]\r\n",
      "  workers_per_gpu: 10\r\n",
      "  batchsize_per_gpu: 128\r\n",
      "schedule:\r\n",
      "#  resume:\r\n",
      "#  load_model: YOUR_MODEL_PATH\r\n",
      "  optimizer:\r\n",
      "    name: SGD\r\n",
      "    lr: 0.1\r\n",
      "    momentum: 0.9\r\n",
      "    weight_decay: 0.0001\r\n",
      "  warmup:\r\n",
      "    name: linear\r\n",
      "    steps: 500\r\n",
      "    ratio: 0.01\r\n",
      "  total_epochs: 190\r\n",
      "  lr_schedule:\r\n",
      "    name: MultiStepLR\r\n",
      "    milestones: [130,160,175,185]\r\n",
      "    gamma: 0.1\r\n",
      "  val_intervals: 5\r\n",
      "evaluator:\r\n",
      "  name: CocoDetectionEvaluator\r\n",
      "  save_key: mAP\r\n",
      "\r\n",
      "log:\r\n",
      "  interval: 10\r\n",
      "\r\n",
      "class_names: ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\r\n",
      "              'train', 'truck', 'boat', 'traffic_light', 'fire_hydrant',\r\n",
      "              'stop_sign', 'parking_meter', 'bench', 'bird', 'cat', 'dog',\r\n",
      "              'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe',\r\n",
      "              'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\r\n",
      "              'skis', 'snowboard', 'sports_ball', 'kite', 'baseball_bat',\r\n",
      "              'baseball_glove', 'skateboard', 'surfboard', 'tennis_racket',\r\n",
      "              'bottle', 'wine_glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\r\n",
      "              'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot',\r\n",
      "              'hot_dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\r\n",
      "              'potted_plant', 'bed', 'dining_table', 'toilet', 'tv', 'laptop',\r\n",
      "              'mouse', 'remote', 'keyboard', 'cell_phone', 'microwave',\r\n",
      "              'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock',\r\n",
      "              'vase', 'scissors', 'teddy_bear', 'hair_drier', 'toothbrush']\r\n"
     ]
    }
   ],
   "source": [
    "cat ../nano-det-parkingline/config/nanodet-g.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import mmcv\n",
    "from nanodet.model.arch import build_model\n",
    "\n",
    "PATH_TO_CONFIG = '../nano-det-parkingline/config/nanodet-g.yml'\n",
    "cfg = yaml.safe_load(open(PATH_TO_CONFIG))\n",
    "cfg = mmcv.Config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish initialize Lite GFL Head.\n"
     ]
    }
   ],
   "source": [
    "model = build_model(cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pytorch-to-tflite/lib/python3.9/site-packages/torch/nn/functional.py:3609: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/pytorch-to-tflite/lib/python3.9/site-packages/torch/nn/functional.py:3657: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "img = torch.randn(1,3,416,416)\n",
    "out = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p cache/\n",
    "onnx_out_path = 'cache/out.onnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pytorch-to-tflite/lib/python3.9/site-packages/torch/onnx/symbolic_helper.py:374: UserWarning: You are trying to export the model with onnx:Upsample for ONNX opset version 9. This operator might cause results to not match the expected results by PyTorch.\n",
      "ONNX's Upsample/Resize operator did not match Pytorch's Interpolation until opset 11. Attributes to determine how to transform the input were added in onnx:Resize in opset 11 to support Pytorch's behavior (like coordinate_transformation_mode and nearest_mode).\n",
      "We recommend using opset 11 and above for models using this operator. \n",
      "  warnings.warn(\"You are trying to export the model with \" + onnx_op + \" for ONNX opset version \"\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, img, onnx_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `__call__` contains input name(s) input.1 with unsupported characters which will be renamed to input_1 in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cache/out.onnx.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cache/out.onnx.tf/assets\n"
     ]
    }
   ],
   "source": [
    "onnx_path = onnx_out_path\n",
    "tf_path = onnx_path + '.tf'\n",
    "onnx_to_tf(onnx_path=onnx_path, tf_path=tf_path)\n",
    "assert os.path.exists(tf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow to tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cache/out.onnx.tf.tflite'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_path = tf_path+'.tflite'\n",
    "tf_to_tf_lite(tf_path, tflite_path)\n",
    "assert os.path.exists(tflite_path)\n",
    "tflite_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
